---
title: "Assignment4_PowerLaws"
author: "Riccardo Fusaroli"
date: "July 12, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Assignment 4 - Assessing Complex Systems via Power Laws

In this assignment you will analyse the finger tapping data you produced last week. From the last two classes we (should) have developed the hypothesis that the self-driven tapping will be the result of a more self-organized system than the metronome driven tapping. This can be operationalized in the following hypotheses:

- H1: The metronome condition will have more normal distributions of interbeat intervals than the self-driven one
- H2: The metronome condition will minimize autocorrelation/mutual information faster than the self-driven one 
- H3: The metronome condition will have fewer power laws than the self-driven one
- H3a: The metronome condition will have lower alphas than the self-driven one
- H4: The metronome condition will have lower alpha (whiter noise) than the self-driven one

In order to test these hypotheses we will first run through the analysis of one time-series (to make sure our code will work) and only then loop the analysis through the full dataset.

In particular we will:

- visualize the distribution
- check for normality
- check for time-dependence (auto-correlation, mi).

- check for power-law distribution!
- check for spectral density (dfa/noise analysis)

- loop through all the time-series
- run mixed effects models to figure out whether:

We will rely on two great R packages: nonlinearTseries and poweRlaw. Both packages have great vignettes that you should use to guide your analyses. Also they have accompanying documents detailing all the theory (slightly technical, though):

- nonlinearTseries: Kantz et al Nonlinear Time Series Analysis
- poweRlaw: Clauset et al (2007) Power Law distributions in empirical data

## Load libraries and data
```{r}

library(pacman)
p_load(nonlinearTseries,poweRlaw,ggplot2,lme4,lmerTest,MuMIn)

# Generating white noise for comparison
values=rnorm(1000, mean = 0, sd = 1)
wNoise=data.frame(values)
# Generating random numbers from a power law
values=scale(rpldis(1000, xmin=1, alpha=1.1, discrete_max = 10000),center=T,scale=T)
pLaw1=data.frame(values)
values=scale(rpldis(1000, xmin=1, alpha=2, discrete_max = 10000),center=T,scale=T)
pLaw2=data.frame(values)
values=scale(rpldis(1000, xmin=1, alpha=3, discrete_max = 10000),center=T,scale=T)
pLaw3=data.frame(values)

```

## Visualizing the distributions

```{r}
ggplot(wNoise,aes(values))+geom_histogram()
ggplot(pLaw1,aes(values))+geom_histogram()
ggplot(pLaw2,aes(values))+geom_histogram()
ggplot(pLaw3,aes(values))+geom_histogram()
```

## Check for normality

Tips: use shapiro and qqnorm (and yes, I know testing for normality has plenty of issues :-))

```{r}
shapiro.test(wNoise$values)
qqnorm(wNoise$values);qqline(wNoise$values, col = 2)
shapiro.test(pLaw1$values)
qqnorm(pLaw1$values);qqline(pLaw1$values, col = 2)
shapiro.test(pLaw2$values)
qqnorm(pLaw2$values);qqline(pLaw2$values, col = 2)
shapiro.test(pLaw3$values)
qqnorm(pLaw3$values);qqline(pLaw3$values, col = 2)
```

## Check for time-dependence

Tips: check nonlinearTseries::timeLag() and read carefully through the nonlinearTseries vignette

```{r}
tau.ami = timeLag(pLaw2$values, technique = "ami", lag.max = 30, do.plot = T)
#emb.dim = estimateEmbeddingDim(pLaw2$values, time.lag = tau.ami,max.embedding.dim = 10)

```

## Check for power-law distribution!

N.B. In order to solve this part of the assignment read carefully through the poweRlaw vignette

```{r}

# Create a new power law object
m_m = conpl$new(pLaw2$values)
m_m$getXmin()
m_m$getPars()

#m_m$setPars(2)
# Estimate xmin (using kolmogorov-smirnoff)
(est = estimate_xmin(m_m))
# Set xmin
m_m$setXmin(est)
# Estimate alpha given an xmin
(est = estimate_pars(m_m))

# Create the data to plot the powerlaw
dd = plot(m_m)
ggplot(dd, aes(x,y))+geom_point() + scale_y_log10()+ scale_x_log10()

# Now estimating uncertainty
cores=parallel::detectCores() # Figuring out how many cores on your machine
bs = bootstrap(m_m, no_of_sims=1000, threads=cores) # Bootstrapping and distributing the bootstrapping across cores
hist(bs$bootstraps[,2], breaks="fd")#xmin
hist(bs$bootstraps[,3], breaks="fd")#alpha
plot(jitter(bs$bootstraps[,2], factor=1.2), bs$bootstraps[,3])
bs_p = bootstrap_p(m_m, no_of_sims=1000, threads=cores)
```

## Check for spectral density (noise analysis)

Tips: check nonlinearTseries::dfa()

```{r}
x1=dfa(wNoise$values)
estimate(x1,do.plot=TRUE)
#x2=rqa(takens = NULL, time.series = pLaw2$values, embedding.dim = emb.dim, time.lag = tau.ami,radius=1,do.plot = T)
```

## Now let's loop the whole process through the whole set of time-series

Remember that we are interested in having a dataframe with the following columns:
- Participant
- Condition
- Normality
- Lag at which autocorrelation is minimized
- Lag at which mutual information is minimized
- Estimated xmin
- Estimated alpha
- Dfa estimate

```{r}

```

## Now let's test a few hypotheses
Per each hypothesis run a statistical test and a draw a plot

- H1: The metronome condition will have more normal distributions of interbeat intervals than the self-driven one
- H2: The metronome condition will minimize autocorrelation/mutual information faster than the self-driven one 
- H3: The metronome condition will have fewer power laws than the self-driven one
- H3a: The metronome condition will have lower alphas than the self-driven one
- H4: The metronome condition will have lower alpha (whiter noise) than the self-driven one

```{r}

```

## Here you need to write a report

METHODS SECTION

blabla

RESULTS SECTION

blabla
